# MP-GCN Playground Group Activity Recognition  
Multi-Person Graph Convolutional Network for Safety-Oriented Activity Classification

This repository contains a full pipeline for **Group Activity Recognition (GAR)** in playground environments using **2D skeletons**, **object context**, and a **Multi-Person Graph Convolutional Network (MP-GCN)**.  
The system takes raw videos, extracts poses, annotates scenes in CVAT, builds panoramic humanâ€“object graphs, and trains an MP-GCN model to classify scenes into:

- **Transit**
- **Play_Object_Normal**
- **Play_Object_Risk**

---

## ğŸš€ Project Overview

Playground environments are complex: multiple people interact simultaneously with static or moving structures (slides, swings, ramps), often with occlusions or irregular motion.  
This project builds an **end-to-end automated pipeline** that transforms raw videos into graph-structured tensors suitable for deep learning models.

The model is based on the MP-GCN architecture proposed by *Li et al., 2024*, adapted to safety-focused playground scenes.

---

## ğŸ“¦ Repository Structure

â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ npz/ # Final graph tensors for training
â”‚ â”œâ”€â”€ intermediate/ # JSON structured pose/object annotations
â”‚ â”œâ”€â”€ cvat_exports/ # Raw CVAT annotation files
â”‚ â””â”€â”€ stats/ # Dataset statistics & plots
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ script.py # Single-video full pipeline: frames â†’ CVAT â†’ poses â†’ JSON
â”‚ â”œâ”€â”€ run_batch_pipeline.py# Batch processing for multiple videos
â”‚ â”œâ”€â”€ extract_job_annotations.py
â”‚ â”œâ”€â”€ cvat_to_intermediate.py
â”‚ â”œâ”€â”€ intermediate_to_npz.py
â”‚ â””â”€â”€ dataset_stats.py
â”‚
â”œâ”€â”€ MPGCN/
â”‚ â”œâ”€â”€ nets.py # MP-GCN model implementation
â”‚ â””â”€â”€ graphs.py # Graph adjacency definitions
â”‚
â”œâ”€â”€ train_mpgcn.py # Full training script
â”œâ”€â”€ utils/ # Helper functions
â””â”€â”€ README.md


---

## ğŸ§© Pipeline Summary

The system consists of **four major stages**:

### **1. Pose Extraction and Tracking**
- Frames extracted at **15 FPS**
- Human poses via **YOLO-Pose**
- Temporal identity via **DeepSort**
- Outputs:
  - 17-joint skeletons (COCO)
  - Consistent person IDs
  - Detected object centroids

### **2. Annotation in CVAT**
- Automatic upload of frames and detections
- Manual scene-level labeling:
  - Transit  
  - Play\_Object\_Normal  
  - Play\_Object\_Risk  
- Optional:
  - roles, safety flags, actions

### **3. Graph Tensor Construction**
Each clip becomes:
X âˆˆ R[ C=2 , T=30 , V'=21 , M=6 ]


Where:
- **C**: coordinates (x,y)
- **T**: frames per clip
- **V'**: 17 human joints + 4 object nodes
- **M**: max persons per clip

### **4. MP-GCN Model**
- Spatial graph convolutions  
- Temporal convolutions  
- Learnable adjacency refinement  
- Person-level attention pooling  
- 3-way softmax classification

---

## ğŸ§  Training the Model

Run:

```bash
python3 train_mpgcn.py \
    --data-dir data/npz \
    --epochs 21 \
    --batch-size 8 \
    --use-augmentation
```

### ğŸ“ Training Logs Include
- Accuracy curves  
- Class-specific behavior  
- Confusion matrix  

---

### ğŸ§ª Dataset Summary

**Final dataset class distribution:**

| Class               | Samples |
|--------------------|---------|
| Transit            | 74      |
| Play_Object_Normal | 25      |
| Play_Object_Risk   | 21      |

Dataset imbalance strongly influences validation metrics, especially between normal vs. risky behavior.

---

### ğŸ“Š Results

- **Training accuracy:** ~0.62  
- **Validation accuracy:** fluctuating due to:  
  - small dataset  
  - class imbalance  
  - subtle pose differences  

**MP-GCN successfully captures:**
- humanâ€“human interactions  
- humanâ€“object interactions  
- group-level motion patterns  

**Key plot examples:**
- `resultsTrainingVal.png`
- `confusionMatrix.png`

---

### ğŸ” Limitations
- Dataset size and imbalance  
- Only 2D pose â€” no depth cues  
- Static object treatment (objects may move)  
- Fine-grained risk labeling is challenging  

---

### ğŸ›  Future Work
- Multi-view or 3D pose recovery  
- Dynamic object nodes  
- Self-supervised pretraining  
- Larger-scale annotated dataset  
- Temporal attention for micro-actions  

---

### ğŸ“š References

**Li, Z., Chang, X., Li, Y., & Su, J. (2024).**  
*Skeleton-Based Group Activity Recognition via Spatial-Temporal Panoramic Graph.*

**Choi, W., Shahid, K., & Savarese, S. (2009).**  
*What are they doing? Collective activity classification using spatio-temporal relationships among people.*

---

### ğŸ‘¨â€ğŸ’» Authors
- **David GÃ³mez** â€“ TecnolÃ³gico de Monterrey  
- **Angela Aguilar** â€“ TecnolÃ³gico de Monterrey  
- **Jorge Reyes** â€“ TecnolÃ³gico de Monterrey  

---

### â­ Acknowledgements
This project was developed as part of a research-oriented course on computational vision and machine learning, applying MP-GCN methods to real-world safety monitoring scenarios.





